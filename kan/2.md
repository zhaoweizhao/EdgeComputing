```python
DYNN:avg=26.8611168255806ms
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   236                                               @profile
   237                                               def forward(self, x):
   238     10100      63182.2      6.3      0.0          B = x.shape[0]
   239     10100       8432.6      0.8      0.0          n_blocks = 12
   240                                                   # print("B:",B)
   241     10100    4653593.2    460.8      1.8          x = self.patch_embed(x)
   242     10100     401628.5     39.8      0.2          cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
   243     10100     294340.6     29.1      0.1          dist_token = self.dist_token.expand(B, -1, -1)
   244     10100     974876.3     96.5      0.4          x = torch.cat((cls_tokens, dist_token, x), dim=1)
   245                                                   # print("X",x.shape)
   246                                                   # print("pos_embed",self.pos_embed.shape)
   247     10100     785276.7     77.8      0.3          x = x + self.pos_embed
   248     10100     598704.2     59.3      0.2          x = self.pos_drop(x)
   249
   250    113256     410562.2      3.6      0.2          for blk_idx, blk in enumerate(self.blocks):
   251    107266  148163388.1   1381.3     56.2              x = blk.forward(x)
   252    107266     206865.6      1.9      0.1              if blk_idx < 11 and blk_idx > 4:
   253     50776    6025846.1    118.7      2.3                  inter_z = self.norm(x)
   254                                                           # inter_logit = (self.intermediate_heads[blk_idx](inter_z[:, 0]) + self.intermediate_heads_dist[blk_idx](inter_z[:, 1])) / 2
   255     50776   10397619.4    204.8      3.9                  inter_logit = self.intermediate_heads[blk_idx](inter_z[:, 0])
   256                                                           # probs = torch.nn.functional.softmax(inter_logit, dim=1)
   257                                                           # print(probs)
   258                                                           # pred, _ = probs.topk(1, 1, True, False)
   259                                                           # print(pred)
   260     50776   78458972.8   1545.2     29.8                  g =self.gates[blk_idx](inter_logit)
   261     50776    3234124.2     63.7      1.2                  g = torch.nn.functional.sigmoid(g)
   262                                                           # print(g)
   263     50776    6948321.7    136.8      2.6                  if g >= self.threshold:
   264                                                               # print(blk_idx)
   265      4110      12385.3      3.0      0.0                      return inter_logit, blk_idx
   266
   267      5990     704235.6    117.6      0.3          x = self.norm(x)
   268                                                   # x = (self.head(x[:, 0]) + self.head_dist(x[:, 1])) / 2
   269      5990    1092973.8    182.5      0.4          x = self.head(x[:, 0])
   270      5990       7655.7      1.3      0.0          return x, blk_idx

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    14                                               @profile
    15                                               def forward(self, logits: Tensor) -> (Tensor):
    16                                                   # p_maxes, entropies, _, margins, entropy_pows = compute_detached_uncertainty_metrics(logits, None)
    17     50787    3504727.0     69.0      5.3          probs = torch.nn.functional.softmax(logits, dim=1)
    18
    19     50787    5247954.3    103.3      7.9          top2prob, _ = torch.topk(probs, 2)
    20     50787    2347254.5     46.2      3.5          p_max = top2prob[:,0]
    21     50787    1794068.7     35.3      2.7          next_p_max = top2prob[:,1]
    22     50787    2695068.7     53.1      4.1          margins = p_max-next_p_max
    23
    24     50787   13711188.9    270.0     20.7          entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)
    25
    26
    27     50787    3413371.6     67.2      5.1          pow_probs = probs**2
    28     50787    5553630.9    109.4      8.4          pow_probs = pow_probs / pow_probs.sum(dim=1, keepdim=True)
    29     50787   12645269.6    249.0     19.0          entropy_pow = -torch.sum(pow_probs * torch.log(pow_probs + 1e-10), dim=1)
    30                                                   # print(entropy_pow)
    31
    32     50787    1023799.2     20.2      1.5          p_maxes = p_max.unsqueeze(1)
    33     50787     661463.5     13.0      1.0          entropies = entropy.unsqueeze(1)
    34     50787     605378.1     11.9      0.9          margins = margins.unsqueeze(1)
    35     50787     572908.5     11.3      0.9          entropy_pows = entropy_pow.unsqueeze(1)
    36     50787    3453328.7     68.0      5.2          uncertainty_metrics = torch.cat((p_maxes, entropies, margins, entropy_pows), dim = 1)
    37     50787    9153319.2    180.2     13.8          return self.linear(uncertainty_metrics.to(logits.device, dtype=logits.dtype))
```
```python
Epoch:5 || acc_layer0_val:28.24
Epoch:5 || acc_layer1_val:36.74
Epoch:5 || acc_layer2_val:40.52
Epoch:5 || acc_layer3_val:43.94
Epoch:5 || acc_layer4_val:44.82
Epoch:5 || acc_layer5_val:60.919999999999995
Epoch:5 || acc_layer6_val:67.13
Epoch:5 || acc_layer7_val:72.83
Epoch:5 || acc_layer8_val:80.67999999999999
Epoch:5 || acc_layer9_val:89.97
Epoch:5 || acc_layer10_val:93.33
Epoch:5 || acc_layer11_val:97.1

****************0.5****************
Exiting Layer0:[0.0/0]
Exiting Layer1:[0.0/0]
Exiting Layer2:[0.0/0]
Exiting Layer3:[0.0/0]
Exiting Layer4:[0.0/0]
Exiting Layer5:[3.569999933242798/98.03921508789062]
Exiting Layer6:[10.920000076293945/97.9853515625]
Exiting Layer7:[10.1899995803833/98.9205093383789]
Exiting Layer8:[18.8700008392334/96.87334442138672]
Exiting Layer9:[23.540000915527344/98.55564880371094]
Exiting Layer10:[16.020000457763672/95.1935043334961]
Exiting Layer11:[16.889999389648438/88.92835998535156]
acc_val=96.03,total_GFLOPs=108.13236236572266G
```
