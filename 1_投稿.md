# Energy-efficient Image Classification on Edge Devices

## Abstract
DNN模型，例如ViT，CNN，在计算机视觉任务中取得了巨大的成功。然而，将大型且计算密集的DNN模型直接部署在资源受限的边缘设备上，会导致难以忍受的能耗和时延问题。
为了解决这个问题，我们提出一个名为GFDE(GFNet,Distil,Early Exit)的新颖框架，该框架通过降低DNN模型的参数量，并允许样本提前退出，从而在边缘端实现了高效节能的图片分类。
具体来说，我们首先通过知识蒸馏对DNN模型进行轻量化，再对蒸馏后的模型加入早退分支，早退分支确定输入样本从哪里退出。此外，在模型推理的时候，我们将早退分支的参数加载到CPU上，
其他参数一律加载到GPU上，实现了简单的异构计算。大量实验结果表明，与经典的深度学习网络相比，我们所提出的框架实现了高达64.74%的能耗降低率和38.87%的时延降低率，同时保证精度损失在3%以内。

## OVERVIEW AND PROBLEM STATEMENT
为了实现latency-efficient和energy-efficienct的inference，我们提出了GFDE框架，该框架利用GFNet作为backbone，同时我们利用DeiT的全局蒸馏思想
对GFNet进行知识蒸馏。最后我们为蒸馏后的模型加入早退分支模块，可以通过辨别样本的复杂度来选择退出点。图1展示了GFDE的总体流程算法。图2展示了GFDE的总体框架图。

1、针对指定数据集，对原始的、大的GFNet模型进行迁移学习。

2、利用DeiT里的全局蒸馏思想对原始的、大的GFNet进行知识蒸馏，得到较小、轻量化的模型。

3、为蒸馏后的模型加入早退分支。

4、在做模型推理时，将早退分支的参数加载到CPU吗，其他参数加载到GPU上。

GFNet是什么？

GFNet是一种transfomer形式的架构，能够以nlogn复杂度学习频域中的长期空间依赖性。GFNet主要由三个关键操作构成：二维离散傅立叶变换、频域特征与可学习全局滤波器之间的逐元素乘法操作、以及二维逆傅立叶变换。Global Filter Layer以对log-linear复杂度混合tokens，受益于高效的快速傅里叶变换(FFT)算法。GFNet由多个GFNet Blocks堆叠组成，每个GFNet Block又由Global Filter Layers 和 Feedforward Networks (FFN)堆叠而成，GFNet Block的网络结构由下图展示：

![image](https://github.com/user-attachments/assets/5a3a6810-36e1-455d-914b-5c27ae5c1989)

知识蒸馏？

知识蒸馏（Knowledge Distillation）是一种模型训练的技术，旨在通过传递一个大型教师模型的知识来训练一个小型学生模型。这个方法的目标是使得学生模型能够获得与教师模型相似的性能，同时减少学生模型的参数量和计算成本。我们从Deit中汲取灵感，利用DeiT里的全局蒸馏思想对我们的GFNet模型做知识蒸馏，从而实现在参数较少的情况下实现高效的图像分类。类似DeiT，我们也额外引入了一个Distillation token，同样，Distillation token类似于Class token，它通过Global Filter与其他embeddings进行交互，并由网络在最后一层之后输出，在网络输出时，其目标是重现教师预测的（硬）标签，而不是真实标签。我们首先预训练出一个大型的、精度高的GFNet模型，利用这个模型作为教师模型，本文采用DeiT的hard-label distillation，将教师模型的hard decision作为true label。

![image](https://github.com/user-attachments/assets/73ee700c-b774-4282-8f50-3133579376a2)


## METHOD
